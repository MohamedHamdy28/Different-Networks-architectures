{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gezb4ovkereX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from collections import OrderedDict\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "od-IHzGCC2sr"
      },
      "source": [
        "# [VGG](https://arxiv.org/pdf/1409.1556.pdf)\n",
        "\n",
        "Implement VGG16, for that write specific `nn.Module`, `VGGBlock` implementing block of VGG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reference: https://github.com/msyim/VGG16/blob/master/VGG16.py\n",
        "def conv_layer(chann_in, chann_out, k_size, p_size):\n",
        "    layer = nn.Sequential(\n",
        "        nn.Conv2d(chann_in, chann_out, kernel_size=k_size, padding=p_size),\n",
        "        nn.BatchNorm2d(chann_out),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    return layer\n",
        "\n",
        "def VGGBlock(in_list, out_list, k_list, p_list, pooling_k, pooling_s):\n",
        "\n",
        "    layers = [ conv_layer(in_list[i], out_list[i], k_list[i], p_list[i]) for i in range(len(in_list)) ]\n",
        "    layers += [ nn.MaxPool2d(kernel_size = pooling_k, stride = pooling_s)]\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def vgg_fc_layer(size_in, size_out):\n",
        "    layer = nn.Sequential(\n",
        "        nn.Linear(size_in, size_out),\n",
        "        nn.BatchNorm1d(size_out),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    return layer\n",
        "\n",
        "class VGG16(nn.Module):\n",
        "    def __init__(self, n_classes=1000):\n",
        "        super(VGG16, self).__init__()\n",
        "\n",
        "        # Conv blocks (BatchNorm + ReLU activation added in each block)\n",
        "        self.layer1 = VGGBlock([3,64], [64,64], [3,3], [1,1], 2, 2)\n",
        "        self.layer2 = VGGBlock([64,128], [128,128], [3,3], [1,1], 2, 2)\n",
        "        self.layer3 = VGGBlock([128,256,256], [256,256,256], [3,3,3], [1,1,1], 2, 2)\n",
        "        self.layer4 = VGGBlock([256,512,512], [512,512,512], [3,3,3], [1,1,1], 2, 2)\n",
        "        self.layer5 = VGGBlock([512,512,512], [512,512,512], [3,3,3], [1,1,1], 2, 2)\n",
        "\n",
        "        # FC layers\n",
        "        self.layer6 = vgg_fc_layer(7*7*512, 4096)\n",
        "        self.layer7 = vgg_fc_layer(4096, 4096)\n",
        "\n",
        "        # Final layer\n",
        "        self.layer8 = nn.Linear(4096, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        vgg16_features = self.layer5(out)\n",
        "        out = vgg16_features.view(out.size(0), -1)\n",
        "        out = self.layer6(out)\n",
        "        out = self.layer7(out)\n",
        "        out = self.layer8(out)\n",
        "\n",
        "        return vgg16_features, out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi43r1dPDRp7"
      },
      "source": [
        "# [GoogLeNet](https://arxiv.org/pdf/1409.4842.pdf)\n",
        "\n",
        "## Inception module\n",
        "\n",
        "Write specific `nn.Module` for Inception module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "L91nFCXhDhxD"
      },
      "outputs": [],
      "source": [
        "# Reference: https://sahiltinky94.medium.com/know-about-googlenet-and-implementation-using-pytorch-92f827d675db\n",
        "\n",
        "class InceptionModule(nn.Module):\n",
        "    def __init__(self, curr_in_fts, f_1x1, f_3x3_r, f_3x3, f_5x5_r, f_5x5, f_pool_proj):\n",
        "        super(InceptionModule, self).__init__()\n",
        "        self.conv1 = ConvBlock(curr_in_fts, f_1x1, 1, 1, 0)\n",
        "        self.conv2 = ReduceConvBlock(curr_in_fts, f_3x3_r, f_3x3, 3, 1)\n",
        "        self.conv3 = ReduceConvBlock(curr_in_fts, f_5x5_r, f_5x5, 5, 2)\n",
        "\n",
        "        self.pool_proj = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=(1, 1), stride=(1, 1)),\n",
        "            nn.Conv2d(in_channels=curr_in_fts, out_channels=f_pool_proj, kernel_size=(1, 1), stride=(1, 1)),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, input_img):\n",
        "        out1 = self.conv1(input_img)\n",
        "        out2 = self.conv2(input_img)\n",
        "        out3 = self.conv3(input_img)\n",
        "        out4 = self.pool_proj(input_img)\n",
        "\n",
        "        x = torch.cat([out1, out2, out3, out4], dim=1)\n",
        "\n",
        "        return x\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_fts, out_fts, k, s, p):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.structure = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_fts, out_channels=out_fts, kernel_size=(k, k), stride=(s, s), padding=(p, p)),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.structure(inputs)\n",
        "\n",
        "        return x\n",
        "\n",
        "class ReduceConvBlock(nn.Module):\n",
        "    def __init__(self, in_fts, out_fts_1, out_fts_2, k, p):\n",
        "        super(ReduceConvBlock, self).__init__()\n",
        "        self.redConv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_fts, out_channels=out_fts_1, kernel_size=(1, 1), stride=(1, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=out_fts_1, out_channels=out_fts_2, kernel_size=(k, k), stride=(1, 1), padding=(p, p)),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, input_img):\n",
        "        x = self.redConv(input_img)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pot5itHXDisN"
      },
      "source": [
        "## Stem network\n",
        "\n",
        "Write down, why do we need a Stem network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhADh2IVDojD"
      },
      "source": [
        "It can help to quickly downsample an input image with strided convolutions of quite large kernel size so that further layers can effectively do their work with much less computational complexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD7dsr2yDuGC"
      },
      "source": [
        "# [ResNet](https://arxiv.org/pdf/1512.03385.pdf)\n",
        "\n",
        "Implement ResNet-18, for that write specific `nn.Module`, `ResNetBlock` implementing block of ResNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FJxA2J7OEPxE"
      },
      "outputs": [],
      "source": [
        "# Reference: https://debuggercafe.com/implementing-resnet18-in-pytorch-from-scratch/\n",
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        stride: int = 1,\n",
        "        expansion: int = 1,\n",
        "        downsample: nn.Module = None\n",
        "    ) -> None:\n",
        "        super(ResNetBlock, self).__init__()\n",
        "        # Multiplicative factor for the subsequent conv2d layer's output channels.\n",
        "        # It is 1 for ResNet18 and ResNet34.\n",
        "        self.expansion = expansion\n",
        "        self.downsample = downsample\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels, \n",
        "            out_channels, \n",
        "            kernel_size=3, \n",
        "            stride=stride, \n",
        "            padding=1,\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            out_channels, \n",
        "            out_channels*self.expansion, \n",
        "            kernel_size=3, \n",
        "            padding=1,\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels*self.expansion)\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return  out\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAkVh_wLES6U"
      },
      "source": [
        "# [ResNeXt](https://arxiv.org/pdf/1611.05431.pdf)\n",
        "\n",
        "Write specific `nn.Module`, `ResNeXtBlock` implementing block of ResNeXt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6FN7E2GADnTI"
      },
      "outputs": [],
      "source": [
        "# Reference: https://github.com/pytorch/vision/blob/1aef87d01eec2c0989458387fa04baebcc86ea7b/torchvision/models/resnet.py#L75\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "class ResNextBlock(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(ResNextBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebgJY9YcEw2U"
      },
      "source": [
        "# [SENet](https://arxiv.org/pdf/1709.01507.pdf)\n",
        "\n",
        "Write specific `nn.Module`, `SEBlock` implementing block of SENet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BfddhrMkE5un"
      },
      "outputs": [],
      "source": [
        "# Reference: https://github.com/moskomule/senet.pytorch/blob/master/senet/se_module.py\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "7fe7e10e5f5d0508c9b62331169c598711bde018a8abe63e5380bb769babbd73"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
